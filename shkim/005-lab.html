<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
005-lab. implement logistic regression(classification) with tensorflow
<xmp>
# To define hypothesis function of logistic regression(classification),
# we use sigmoid function
$$$H(X) = \frac{1}{1+e^{-W^{T}X}}$$$

# Loss function of logistic regression, whose shape becomes convex with combined graph
$$$Loss(W) = -\frac{1}{m} \sum y\log{(H(x))} + (1-y)(\log{(1-H(x))})$$$
# We can update Weight by using gradient descent algorithm
$$$W := W - \alpha \frac{\partial}{\partial{W}} Loss(W)$$$

@
# Lab 5 Logistic Regression Classifier
import tensorflow as tf
tf.set_random_seed(777)
# study time $$$x_{1}$$$
# study time $$$x_{2}$$$
x_data = [[1, 2],
          [2, 3],
          [3, 1],
          [4, 3],
          [5, 3],
          [6, 2]]
# y is pass or fail
y_data = [[0],
          [0],
          [0],
          [1],
          [1],
          [1]]

# Placeholder nodes
# None: Many instances
# 2: number of features
X = tf.placeholder(tf.float32, shape=[None, 2])
# None: Many instances
# 1: number of label
Y = tf.placeholder(tf.float32, shape=[None, 1])

# Parameter nodes
# XW=H(X)
# [n,2][?,?]=[n,1]
# [?,?]=[2,1]
W = tf.Variable(tf.random_normal([2, 1]), name='weight')
# Shape of b is same with number of column of y
b = tf.Variable(tf.random_normal([1]), name='bias')

# Hypothesis function of logistic regression is sigmoid function:
# hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(X, W)))
hypothesis = tf.sigmoid(tf.matmul(X, W) + b)

# loss function
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
# We apply gradient descent algorithm on loss function
# to find optimized parameters W and b of hypothesis function
train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

# If prediction(hypothesis) is obtained as 0.8, it can be said as pass(1)
# If prediction(hypothesis) is obtained as 0.2, it can be said as fail(0)
# Standard borderline can be 0.5
# predicted is either 1.0 or 0.0
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
# tf.equal(predicted, Y): true or false

accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

# Open session stream
with tf.Session() as sess:
    Initialize TensorFlow variables(W, b)
    sess.run(tf.global_variables_initializer())

    for step in range(10001):
        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
        if step % 200 == 0:
            print(step, cost_val)

    # Pass data for test and get accuracy report
    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})
    print("\nHypothesis:\n", h, "\nCorrect (Y):\n", c, "\nAccuracy:\n", a)

# 0 1.73078
# 200 0.571512
# 400 0.507414
# 600 0.471824
# 800 0.447585
# ...
# 9200 0.159066
# 9400 0.15656
# 9600 0.154132
# 9800 0.151778
# 10000 0.149496

# Hypothesis:
#  [[ 0.03074029]
#  [ 0.15884677]
#  [ 0.30486736]
#  [ 0.78138196]
#  [ 0.93957496]
#  [ 0.98016882]]
# Correct (Y):  
#  [[ 0.]
#  [ 0.]
#  [ 0.]
#  [ 1.]
#  [ 1.]
#  [ 1.]]
# Accuracy:
# 1.0

    
# @
# lab-05-2-logistic_regression_diabetes.py

# Lab 5 Logistic Regression Classifier
import tensorflow as tf
import numpy as np
tf.set_random_seed(777)
# data-03-diabetes.csv
# -0.294118	0.487437	0.180328	-0.292929	0	0.00149028	-0.53117	-0.0333333	0
# -0.882353	-0.145729	0.0819672	-0.414141	0	-0.207153	-0.766866	-0.666667	1
# -0.0588235	0.839196	0.0491803	0	0	-0.305514	-0.492741	-0.633333	0
# -0.882353	-0.105528	0.0819672	-0.535354	-0.777778	-0.162444	-0.923997	0	1

xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)
# Select entire row, select 0:-1 column
x_data = xy[:, 0:-1]
# Select entire row, select [-1] column
y_data = xy[:, [-1]]

print(x_data.shape, y_data.shape)

# None: many instances
# 8: number of features of x_data
X = tf.placeholder(tf.float32, shape=[None, 8])
# None 
# 1
Y = tf.placeholder(tf.float32, shape=[None, 1])

# XW = H(X)
# [n,8][?,?]=[n,1]
# [?,?]=[8,1]
W = tf.Variable(tf.random_normal([8, 1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

hypothesis = tf.sigmoid(tf.matmul(X, W) + b)

# Loss function
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))

train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

with tf.Session() as sess:
    # Initialize TensorFlow variables
    sess.run(tf.global_variables_initializer())

    for step in range(10001):
        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
        if step % 200 == 0:
            print(step, cost_val)

    # Accuracy report
    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})
    print("\nHypothesis:\n", h, "\nCorrect (Y):\n", c, "\nAccuracy:\n", a)

# 0 0.82794
# 200 0.755181
# 400 0.726355
# 600 0.705179
# 800 0.686631
# ...
# 9600 0.492056
# 9800 0.491396
# 10000 0.490767
# ...
#  [ 1.]
#  [ 1.]
#  [ 1.]]
# Accuracy:
# 0.762846


</xmp>
   </BODY>
</HTML>
