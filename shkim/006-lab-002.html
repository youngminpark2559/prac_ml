<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
006-lab-002. one hot encoding, fancy softmax classification
<xmp>
@
lab-06-2-softmax_zoo_classifier.py

import tensorflow as tf
import numpy as np
tf.set_random_seed(777)

# 1,0,0,1,0,0,1,1,1,1,0,0,4,0,0,1,0
# 1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,0
# 0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,3

# Model will predict animal type basedd on various features
xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)
x_data = xy[:, 0:-1]
y_data = xy[:, [-1]]
print(x_data.shape, y_data.shape)

# 0 ~ 6
nb_classes = 7  
# [n,16]
X = tf.placeholder(tf.float32, [None, 16])
# [n,1]
# We have to convert 0-6 classes into one hot encoding
Y = tf.placeholder(tf.int32, [None, 1])
# We use tf.one_hot()
Y_one_hot = tf.one_hot(Y, nb_classes)

# Note
# We have y_data like this form [[0],[3],...,[0]], this is 2 dimension, shape=(?,1)
# When we convert y_data into one hot encoding, shape becomes like this 3 dimension, shape=(?,1,17)
# [[[1 0 0 0 0 0 0]],
# [[0 0 0 1 0 0 0]],
# ...,
# [[1 0 0 0 0 0 0]]]
# But, we want this shape=(?,7)
# To achieve this, we use reshape()

print("one_hot", Y_one_hot)
# Y_one_hot: raw one
# 2nd argument: shape we want
# -1=None
# In conclusion, (-1, 7)
# Before reshape()
# [[[1 0 0 0 0 0 0]],
# [[0 0 0 1 0 0 0]],
# ...,
# [[1 0 0 0 0 0 0]]]
# After reshape()
# [[1 0 0 0 0 0 0],
# [0 0 0 1 0 0 0],
# ...,
# [1 0 0 0 0 0 0]]
Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])
print("reshape", Y_one_hot)

# [n,16][?,?]=[n,7]
# [?,?]=[16,7]
W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')
b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

logits = tf.matmul(X, W) + b
hypothesis = tf.nn.softmax(logits)

# loss function(cross entropy function)
# We will use tf.nn.softmax_cross_entropy_with_logits()
# logits = tf.matmul(X,W)+b
# hypothesis = tf.nn.softmax(logits)
cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)
# This is final form of loss function
# This is identical with cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))
cost = tf.reduce_mean(cost_i)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

prediction = tf.argmax(hypothesis, 1)
correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for step in range(2000):
        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})
        if step % 100 == 0:
            loss, acc = sess.run([cost, accuracy], feed_dict={
                                 X: x_data, Y: y_data})
            print("Step: {:5}\tLoss: {:.3f}\tAcc: {:.2%}".format(
                step, loss, acc))

# Let's see if we can predict well
pred = sess.run(prediction, feed_dict={X: x_data})
# flatten(): [[1],[0]] -> [1,0]

for p, y in zip(pred, y_data.flatten()):
    print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)))

# Step:     0 Loss: 5.106 Acc: 37.62%
# Step:   100 Loss: 0.800 Acc: 79.21%
# Step:   200 Loss: 0.486 Acc: 88.12%
# Step:   300 Loss: 0.349 Acc: 90.10%
# Step:   400 Loss: 0.272 Acc: 94.06%
# Step:   500 Loss: 0.222 Acc: 95.05%
# Step:   600 Loss: 0.187 Acc: 97.03%
# Step:   700 Loss: 0.161 Acc: 97.03%
# Step:   800 Loss: 0.140 Acc: 97.03%
# Step:   900 Loss: 0.124 Acc: 97.03%
# Step:  1000 Loss: 0.111 Acc: 97.03%
# Step:  1100 Loss: 0.101 Acc: 99.01%
# Step:  1200 Loss: 0.092 Acc: 100.00%
# Step:  1300 Loss: 0.084 Acc: 100.00%
# ...
# [True] Prediction: 0 True Y: 0
# [True] Prediction: 0 True Y: 0
# [True] Prediction: 3 True Y: 3
# [True] Prediction: 0 True Y: 0
# [True] Prediction: 0 True Y: 0
# [True] Prediction: 0 True Y: 0
# [True] Prediction: 0 True Y: 0
# [True] Prediction: 3 True Y: 3
# [True] Prediction: 3 True Y: 3
# [True] Prediction: 0 True Y: 0
   
</xmp>
   </BODY>
</HTML>
