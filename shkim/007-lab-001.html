<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"],
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
007-lab-001. learning rate,linear regression with not normalized data,linear regression with normalized data by MinMaxScaler() 
<xmp>
import tensorflow as tf
tf.set_random_seed(777)

x_train_data=[[1,2,1],
              [1,3,2],
              [1,3,4],
              [1,5,5],
              [1,7,5],
              [1,2,5],
              [1,6,6],
              [1,7,7]]
y_train_data=[[0,0,1],
              [0,0,1],
              [0,0,1],
              [0,1,0],
              [0,1,0],
              [0,1,0],
              [1,0,0],
              [1,0,0]]

# We will evaluate our model by following test dataset
x_test_data=[[2,1,1],
             [3,1,2],
             [3,3,4]]
y_test_data=[[0,0,1],
             [0,0,1],
             [0,0,1]]

# [n,3]
X_placeholder_node=tf.placeholder("float",[None,3])

# [n,3]
Y_placeholder_node=tf.placeholder("float",[None,3])

# [n,3][?,?]=[n,3]
# [?,?]=[3,3]
W_variable_node=tf.Variable(tf.random_normal([3,3]))

# [3] is from [None,3]
b_variable_node=tf.Variable(tf.random_normal([3]))

h_f_softmax_node=tf.nn.softmax(tf.matmul(X_placeholder_node,W_variable_node)+b_variable_node)

cost_f_cross_entropy_node=tf.reduce_mean(-tf.reduce_sum(Y_placeholder_node*tf.log(h_f_softmax_node),axis=1))

# You try this model with small learning rate
gradient_descent_optimizer=tf.train.GradientDescentOptimizer(learning_rate=1e-10)\
    .minimize(cost_f_cross_entropy_node)

prediction_after_argmax_node=tf.arg_max(h_f_softmax_node,1)
compare_prediction_and_label_node=tf.equal(prediction_after_argmax_node,tf.arg_max(Y_placeholder_node,1))
accuracy_node=tf.reduce_mean(tf.cast(compare_prediction_and_label_node,tf.float32))

with tf.Session() as sess_object:
    sess_object.run(tf.global_variables_initializer())

    for step in range(201):
        if step%50==0:
            # You use train dataset
            loss_value,W_value,_=sess_object.run([cost_f_cross_entropy_node,W_variable_node,gradient_descent_optimizer]\
                ,feed_dict={X_placeholder_node:x_train_data,Y_placeholder_node:y_train_data})
            print("step:",step,"loss_value:",loss_value,"W_value:",W_value)

    # You let model to predict with test dataset
    print("prediction:",sess_object.run(prediction_after_argmax_node,feed_dict={X_placeholder_node:x_test_data}))
    # I calculate accuracy_node
    print("accuracy_node: ",sess_object.run(accuracy_node,feed_dict={X_placeholder_node: x_test_data,Y_placeholder_node:y_test_data}))

# step: 0 loss_value: 5.7320304 W_value: [[ 0.8026957  0.6786129 -1.2172831]
#  [-0.3051686 -0.3032113  1.508257 ]
#  [ 0.7572236 -0.7008909 -2.108204 ]]
# step: 50 loss_value: 5.7320304 W_value: [[ 0.8026957  0.6786129 -1.2172831]
#  [-0.3051686 -0.3032113  1.508257 ]
#  [ 0.7572236 -0.7008909 -2.108204 ]]
# step: 100 loss_value: 5.7320304 W_value: [[ 0.8026957  0.6786129 -1.2172831]
#  [-0.3051686 -0.3032113  1.508257 ]
#  [ 0.7572236 -0.7008909 -2.108204 ]]
# step: 150 loss_value: 5.7320304 W_value: [[ 0.8026957  0.6786129 -1.2172831]
#  [-0.3051686 -0.3032113  1.508257 ]
#  [ 0.7572236 -0.7008909 -2.108204 ]]
# step: 200 loss_value: 5.7320304 W_value: [[ 0.8026957  0.6786129 -1.2172831]
#  [-0.3051686 -0.3032113  1.508257 ]
#  [ 0.7572236 -0.7008909 -2.108204 ]]
# prediction: [0 0 0]
# accuracy_node:  0.0

# @
import tensorflow as tf
import numpy as np
tf.set_random_seed(777)


xy_train_data=np.array([[828.659973,833.450012,908100,828.349976,831.659973],
               [823.02002,828.070007,1828100,821.655029,828.070007],
               [819.929993,824.400024,1438100,818.97998,824.159973],
               [816,820.958984,1008100,815.48999,819.23999],
               [819.359985,823,1188100,818.469971,818.97998],
               [819,823,1198100,816,820.450012],
               [811.700012,815.25,1098100,809.780029,813.669983],
               [809.51001,816.659973,1398100,804.539978,809.559998]])

x_train_data=xy_train_data[:,0:-1]
y_train_data=xy_train_data[:,[-1]]

X_placeholder_node=tf.placeholder(tf.float32,shape=[None,4])
Y_placeholder_node=tf.placeholder(tf.float32,shape=[None,1])

W_variable_node=tf.Variable(tf.random_normal([4,1]),name='weight')
b_variable_node=tf.Variable(tf.random_normal([1]),name='bias')

h_f_softmax_node=tf.matmul(X_placeholder_node,W_variable_node)+b_variable_node

# This is simplified loss function
simplified_cost_f_node=tf.reduce_mean(tf.square(h_f_softmax_node-Y_placeholder_node))

gradient_descent_optimizer=tf.train.GradientDescentOptimizer(learning_rate=1e-5)

to_be_trained_node=gradient_descent_optimizer.minimize(simplified_cost_f_node)

sess_object=tf.Session()
sess_object.run(tf.global_variables_initializer())

for step in range(101):
    if step%20==0:
        cost_value_in_for,h_f_value,_=sess_object.run(
            [simplified_cost_f_node,h_f_softmax_node,to_be_trained_node]\
                ,feed_dict={X_placeholder_node:x_train_data,Y_placeholder_node:y_train_data})
        print("step:",step,"cost value:",simplified_cost_f_node,"\nPrediction:\n",h_f_value)


lab-07-3-linear_regression_min_max.py
import tensorflow as tf
import numpy as np
tf.set_random_seed(777)

# This method normalizes data
def MinMaxScaler(data):
    numerator=data - np.min(data,0)
    denominator=np.max(data,0) - np.min(data,0)
    # noise term prevents the zero division
    return numerator / (denominator + 1e-7)


xy_train_data=np.array([[828.659973,833.450012,908100,828.349976,831.659973],
               [823.02002,828.070007,1828100,821.655029,828.070007],
               [819.929993,824.400024,1438100,818.97998,824.159973],
               [816,820.958984,1008100,815.48999,819.23999],
               [819.359985,823,1188100,818.469971,818.97998],
               [819,823,1198100,816,820.450012],
               [811.700012,815.25,1098100,809.780029,813.669983],
               [809.51001,816.659973,1398100,804.539978,809.559998]])

# very important. It does not work without it.
xy_train_data_scaled=MinMaxScaler(xy_train_data)
# array([[0.99999999, 0.99999999, 0.        , 1.        , 1.        ],
#        [0.70548491, 0.70439552, 1.        , 0.71881782, 0.83755791],
#        [0.54412549, 0.50274824, 0.57608696, 0.606468  , 0.6606331 ],
#        [0.33890353, 0.31368023, 0.10869565, 0.45989134, 0.43800918],
#        [0.51436   , 0.42582389, 0.30434783, 0.58504805, 0.42624401],
#        [0.49556179, 0.42582389, 0.31521739, 0.48131134, 0.49276137],
#        [0.11436064, 0.        , 0.20652174, 0.22007776, 0.18597238],
#        [0.        , 0.07747099, 0.5326087 , 0.        , 0.        ]])

x_train_data=xy_train_data_scaled[:,0:-1]
y_train_data=xy_train_data_scaled[:,[-1]]

# [n,4]
X_placeholder_node=tf.placeholder(tf.float32,shape=[None,4])

# [n,1]
Y_placeholder_node=tf.placeholder(tf.float32,shape=[None,1])

# [n,4][?,?]=[n,1]
# [?,?]=[4,1]
W_variable_node=tf.Variable(tf.random_normal([4,1]),name='weight')
b_variable_node=tf.Variable(tf.random_normal([1]),name='bias')

hypothesis_function_node=tf.matmul(X_placeholder_node,W_variable_node)+b_variable_node

simplified_cost_f_node=tf.reduce_mean(tf.square(hypothesis_function_node-Y_placeholder_node))

gradient_descent_optimizer=tf.train.GradientDescentOptimizer(learning_rate=1e-5)

to_be_trained_node=gradient_descent_optimizer.minimize(simplified_cost_f_node)

sess_object=tf.Session()
sess_object.run(tf.global_variables_initializer())

for step in range(101):
    if step%20==0:
        cost_value,hypothesis_value,_=sess_object.run(
            [simplified_cost_f_node,hypothesis_function_node,to_be_trained_node]\
                ,feed_dict={X_placeholder_node:x_train_data,Y_placeholder_node:y_train_data})
        print(step,"Cost: ",cost_value,"\nPrediction:\n",hypothesis_value)

# 0 Cost:  0.10422044 
# Prediction:
#  [[0.91121554]
#  [0.88515174]
#  [0.8293996 ]
#  [0.72472095]
#  [0.82365894]
#  [0.75689733]
#  [0.6798932 ]
#  [0.49118808]]
# 20 Cost:  0.104216896 
# Prediction:
#  [[0.91120636]
#  [0.8851419 ]
#  [0.8293911 ]
#  [0.72471416]
#  [0.82365113]
#  [0.7568897 ]
#  [0.6798872 ]
#  [0.49118185]]
# 40 Cost:  0.10421333 
# Prediction:
#  [[0.9111972 ]
#  [0.8851321 ]
#  [0.82938266]
#  [0.72470725]
#  [0.8236433 ]
#  [0.7568821 ]
#  [0.6798811 ]
#  [0.4911756 ]]
# 60 Cost:  0.10420978 
# Prediction:
#  [[0.91118807]
#  [0.8851222 ]
#  [0.8293742 ]
#  [0.72470033]
#  [0.8236356 ]
#  [0.75687444]
#  [0.6798751 ]
#  [0.4911694 ]]
# 80 Cost:  0.104206234 
# Prediction:
#  [[0.91117895]
#  [0.8851123 ]
#  [0.82936573]
#  [0.7246935 ]
#  [0.82362777]
#  [0.7568668 ]
#  [0.67986906]
#  [0.49116313]]
# 100 Cost:  0.10420268 
# Prediction:
#  [[0.91116977]
#  [0.8851025 ]
#  [0.82935727]
#  [0.7246866 ]
#  [0.82361996]
#  [0.7568592 ]
#  [0.679863  ]
#  [0.4911569 ]]

    
</xmp>
   </BODY>
</HTML>
