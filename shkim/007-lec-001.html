<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
007-lec-001. learning rate, preprocess data, overfitting, regularization
<xmp>
@
learning rate is step size in gradient descent algorithm
Too big learning rate can cause overshooting
Too small learning rate can cause too long learning time or local minimum

Proper starting value of learning rate is 0.01

@
Even with good learning rate, overshooting can be occurred, if loss function shapes wide width and short height
In this case, we need to normalize data
There are 2 ways for that(zero-centered data, normalized data)

Standardization(one of normalization techniques):
$$$x_{j}^{'} = \frac{x_{j}-\mu_{j}}{\sigma_{j}}$$$
X_std[:,0] = (X[:,0] - X[:,0].mean() / X[:,0].std())

@
Solutions for overfitting
1. More training data
1. Reduce number of features
1. Regularization
i: training set
$$$\lambda$$$: regularization strength
LossFunction = $$$\frac{1}{N} \sum\limits_{i} D(S(Wx + b), L_{i}) + \lambda \sum W^{2}$$$

Regularization term in tf:
l2regularization = 0.001 * tf.reduce_sum(tf.square(W))
   
</xmp>
   </BODY>
</HTML>
