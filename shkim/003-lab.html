<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
003-lab. linear regression, loss function, gradient descent
<xmp>
# @
# lab-03-1-minimizing_cost_show_graph.py

import tensorflow as tf
import matplotlib.pyplot as plt
# for reproducibility
tf.set_random_seed(777)

# train dataset node
X = [1, 2, 3]
Y = [1, 2, 3]

# Weight node
W = tf.placeholder(tf.float32)

# hypothesis function node
hypothesis = X * W

# loss function node
cost = tf.reduce_mean(tf.square(hypothesis - Y))

# I create session object
sess = tf.Session()

# List holders for weight history and loss history
W_history = []
cost_history = []


for i in range(-30, 50):
    # Weight moves between -3 and 5  
    curr_W = i * 0.1
    # I run model and get loss
    curr_cost = sess.run(cost, feed_dict={W: curr_W})
    # I append weight history and loss history into each list
    W_history.append(curr_W)
    cost_history.append(curr_cost)

plt.plot(W_history, cost_history)
plt.show()


# @
# lab-03-2-minimizing_cost_gradient_update.py

import tensorflow as tf
tf.set_random_seed(777)

x_data = [1, 2, 3]
y_data = [1, 2, 3]

W = tf.Variable(tf.random_normal([1]), name='weight')

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

hypothesis = X * W

cost = tf.reduce_mean(tf.square(hypothesis - Y))

# We manually implement gradient descent algolithm without using tensorflow method
learning_rate = 0.1
gradient = tf.reduce_mean((W * X - Y) * X)
# I get new Weight
descent = W - learning_rate * gradient
# I need to assign new Weight into update
update = W.assign(descent)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(21):
    sess.run(update, feed_dict={X: x_data, Y: y_data})
    print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))
# 0 1.93919 [ 1.64462376]
# 1 0.551591 [ 1.34379935]
# 2 0.156897 [ 1.18335962]
# 3 0.0446285 [ 1.09779179]
# 4 0.0126943 [ 1.05215561]
# 5 0.00361082 [ 1.0278163]
# 6 0.00102708 [ 1.01483536]
# 7 0.000292144 [ 1.00791216]
# 8 8.30968e-05 [ 1.00421977]
# 9 2.36361e-05 [ 1.00225055]
# 10 6.72385e-06 [ 1.00120032]
# 11 1.91239e-06 [ 1.00064015]
# 12 5.43968e-07 [ 1.00034142]
# 13 1.54591e-07 [ 1.00018203]
# 14 4.39416e-08 [ 1.00009704]
# 15 1.24913e-08 [ 1.00005174]
# 16 3.5322e-09 [ 1.00002754]
# 17 9.99824e-10 [ 1.00001466]
# 18 2.88878e-10 [ 1.00000787]
# 19 8.02487e-11 [ 1.00000417]
# 20 2.34053e-11 [ 1.00000226]


# @
# lab-03-3-minimizing_cost_tf_optimizer.py

import tensorflow as tf
tf.set_random_seed(777)

X = [1, 2, 3]
Y = [1, 2, 3]

W = tf.Variable(5.0)

hypothesis = X * W

cost = tf.reduce_mean(tf.square(hypothesis - Y))

# We use tensorflow method without manually implementing formular
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(100):
    print(step, sess.run(W))
    sess.run(train)
# 0 5.0
# 1 1.26667
# 2 1.01778
# 3 1.00119
# 4 1.00008
# ...
# 96 1.0
# 97 1.0
# 98 1.0
# 99 1.0

# @
# lab-03-X-minimizing_cost_tf_gradient.py

import tensorflow as tf
tf.set_random_seed(777)

X = [1, 2, 3]
Y = [1, 2, 3]

W = tf.Variable(5.)

hypothesis = X * W

gradient = tf.reduce_mean((W * X - Y) * X) * 2

cost = tf.reduce_mean(tf.square(hypothesis - Y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

# First, I want to find gradient values about cost
# After getting gradient values, I can edit them
gvs = optimizer.compute_gradients(cost, [W])
# And then I can pass edited gvs
apply_gradients = optimizer.apply_gradients(gvs)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(100):
    print(step, sess.run([gradient, W, gvs]))
    sess.run(apply_gradients)
    
# # Apply gradients
# 0 [37.333332, 5.0, [(37.333336, 5.0)]]
# 1 [33.848888, 4.6266665, [(33.848888, 4.6266665)]]
# 2 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]
# 3 [27.825287, 3.9812808, [(27.825287, 3.9812808)]]
# 4 [25.228262, 3.703028, [(25.228264, 3.703028)]]
# ...
# 96 [0.0030694802, 1.0003289, [(0.0030694804, 1.0003289)]]
# 97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]
# 98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]
# 99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]
      </xmp>
   </BODY>
</HTML>
