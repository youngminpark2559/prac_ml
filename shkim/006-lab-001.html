<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
006-lab-001. softmax function for multinomial classification by tf
<xmp>
# @
# Softmax function is useful when you predict from muliple classes
# Binomial classification predicts 0 or 1

# @
# lab-06-1-softmax_classifier.py

import tensorflow as tf
tf.set_random_seed(777)
# [n,4]
x_data = [[1, 2, 1, 1],
          [2, 1, 3, 2],
          [3, 1, 3, 4],
          [4, 1, 5, 5],
          [1, 7, 5, 5],
          [1, 2, 5, 6],
          [1, 6, 6, 6],
          [1, 7, 7, 7]]
# [n,3]
# We express multiple classses in one hot encoding
# class '0', '1', '2'
# Make 3 spaces
# [ , , ]
# '0' = [1, 0, 0]
# '1' = [0, 1, 0]
# '2' = [0, 0, 1]
# y_data before one hot encoding = [2,2,2,1,1,1,0,0]
y_data = [[0, 0, 1],
          [0, 0, 1],
          [0, 0, 1],
          [0, 1, 0],
          [0, 1, 0],
          [0, 1, 0],
          [1, 0, 0],
          [1, 0, 0]]

X = tf.placeholder("float", [None, 4])
Y = tf.placeholder("float", [None, 3])
nb_classes = 3

# [n,4][?,?]=[n,3]
# [?,?]=[4,3]
W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')
b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

# XW = yhat(=scores=logits)
# tf.matmul(X,W)+b = yhat(=scores=logits)
# softmax = exp(logits) / reduce_sum(exp(logits), dim)
# Hypothesis function for multinomial classification
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

# Loss function of 'hypothesis function for multinomial classification' is cross entropy function
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(2001):
        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})
        if step % 200 == 0:
            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))
    print('--------------')  

# After training, we can test model  
a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})
print(a, sess.run(tf.argmax(a, 1)))
print('--------------')

b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})
print(b, sess.run(tf.argmax(b, 1)))
print('--------------')

c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})
print(c, sess.run(tf.argmax(c, 1)))
print('--------------')

all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})
print(all, sess.run(tf.argmax(all, 1)))

# --------------
# [[  1.38904958e-03   9.98601854e-01   9.06129117e-06]] [1]
# --------------
# [[ 0.93119204  0.06290206  0.0059059 ]] [0]
# --------------
# [[  1.27327668e-08   3.34112905e-04   9.99665856e-01]] [2]
# --------------
# [[  1.38904958e-03   9.98601854e-01   9.06129117e-06]
#  [  9.31192040e-01   6.29020557e-02   5.90589503e-03]
#  [  1.27327668e-08   3.34112905e-04   9.99665856e-01]] [1 0 2]

    
</xmp>
   </BODY>
</HTML>
