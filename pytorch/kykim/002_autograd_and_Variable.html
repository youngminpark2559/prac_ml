<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 20px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 100px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
img {
 width:900px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
<xmp>
import torch
from torch.autograd import Variable
================================================================================
Forward propagation and back propagation
2018-06-17 08-12-25.png
================================================================================
torch.autograd calculates differentiation (gradient) values on behalf of you
================================================================================
Variable autograd.Variable is composed of "data", "grad", "grad_fn"

"data" is data which is stored is torch.autograd.Variable type variable

"grad" is calculated gradient value

When you calculate gradient of "data" wrt to operation, 
operation information is stored in "grad_fn"
================================================================================
Suppose following computational graph structure

1. input_tensor 
(you don't need to use requires_grad=True on input data 
like input image unless you need it explicitly)

2. layer1: $$$\text{output_of_layer_1}=\text{required_grad_is_True_and_randomly_initialized_trainable_variable_1}\times \text{input_tensor}+1$$$

3. loss function: $$$\text{calculated_loss_value}=\text{output_of_layer_1}+100$$$

When input_image is passed through layer1,
"grad_fn" has the operation which is specified in layer1
================================================================================
Your ultimate goal is to update all trainable parameters in all layers
to make all trainable parameters to reflect the pattern of your big data.

Updating trainable parameters is problem which has direction
just like mathematical vector than scalar.

And that direction for updating trainable parameters is provided
from your situation where you need to minimize gradient value $$$\frac{\partial loss}{\partial \text{param_layer1}}$$$
from following gradient descent algorithm 
$$$\text{adjusted_new_param_in_layer1} \leftarrow \text{current_param_in_layer1} - \text{learning_rate} \times \dfrac{\partial loss}{\partial \text{current_param_in_layer1}}$$$
================================================================================
</xmp>
<!-- # @ c input_image: torch tensor as input image
input_image=torch.ones(2,2)
print(input_image)
# tensor([[1., 1.],
#         [1., 1.]])
# @ c trainable_param_in_layer1: trainable parameter in layer1, which is initialized by 1s
trainable_param_in_layer1=input_tensor=torch.ones(2,2)
print(input_tensor)
# tensor([[1., 1.],
#         [1., 1.]]) -->
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #888888"># @ c input_image: torch tensor as input image</span>
input_image<span style="color: #333333">=</span>torch<span style="color: #333333">.</span>ones(<span style="color: #0000DD; font-weight: bold">2</span>,<span style="color: #0000DD; font-weight: bold">2</span>)
<span style="color: #007020">print</span>(input_image)
<span style="color: #888888"># tensor([[1., 1.],</span>
<span style="color: #888888">#         [1., 1.]])</span>

<span style="color: #888888"># @ c trainable_param_in_layer1: trainable parameter in layer1, which is initialized by 1s</span>
trainable_param_in_layer1<span style="color: #333333">=</span>input_tensor<span style="color: #333333">=</span>torch<span style="color: #333333">.</span>ones(<span style="color: #0000DD; font-weight: bold">2</span>,<span style="color: #0000DD; font-weight: bold">2</span>)
<span style="color: #007020">print</span>(input_tensor)
<span style="color: #888888"># tensor([[1., 1.],</span>
<span style="color: #888888">#         [1., 1.]])</span>
</pre></div>
<xmp>
================================================================================
@ As constant tensor input_image goes through all layers, 
parameters as torch.autograd.Variables in all layers need to be defined as requires_grad=True
to tract gradient values.

It means gradient of the loss wrt each trainable parameter is asked to be calculated

@ torch.autograd.Variables parameters in CNN and RNN are defined 
with requires_grad=True by default
================================================================================
</xmp>
<!-- # @ c trainable_param_in_layer1: trainable parameter in layer1 
# with option requires_grad=True
trainable_param_in_layer1=Variable(trainable_param_in_layer1,requires_grad=True)
print(trainable_param_in_layer1)
# tensor([[1., 1.],
#         [1., 1.]], requires_grad=True)
print(trainable_param_in_layer1.data)
# tensor([[1., 1.],
#         [1., 1.]])
print(trainable_param_in_layer1.grad)
# None
# because trainable_param_in_layer1 hadn't performed operation yet
print(trainable_param_in_layer1.grad_fn)
# None
# because trainable_param_in_layer1 hadn't performed operation yet
# @ Create layer1 by using trainable_param_in_layer1
# layer1: (trainable_param_in_layer1*x)+2
# @ Pass input_image into layer1 and get output_of_layer_1
output_of_layer_1=(tensor_after_layer1*input_image)+2
# @ Create loss function layer
loss: loss_value=output_of_layer_1+100 -->
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #888888"># @ c trainable_param_in_layer1: trainable parameter in layer1 </span>
<span style="color: #888888"># with option requires_grad=True</span>
trainable_param_in_layer1<span style="color: #333333">=</span>Variable(trainable_param_in_layer1,requires_grad<span style="color: #333333">=</span><span style="color: #008800; font-weight: bold">True</span>)
<span style="color: #007020">print</span>(trainable_param_in_layer1)
<span style="color: #888888"># tensor([[1., 1.],</span>
<span style="color: #888888">#         [1., 1.]], requires_grad=True)</span>

<span style="color: #007020">print</span>(trainable_param_in_layer1<span style="color: #333333">.</span>data)
<span style="color: #888888"># tensor([[1., 1.],</span>
<span style="color: #888888">#         [1., 1.]])</span>

<span style="color: #007020">print</span>(trainable_param_in_layer1<span style="color: #333333">.</span>grad)
<span style="color: #888888"># None</span>
<span style="color: #888888"># because trainable_param_in_layer1 hadn&#39;t performed operation yet</span>

<span style="color: #007020">print</span>(trainable_param_in_layer1<span style="color: #333333">.</span>grad_fn)
<span style="color: #888888"># None</span>
<span style="color: #888888"># because trainable_param_in_layer1 hadn&#39;t performed operation yet</span>

<span style="color: #888888"># @ Create layer1 by using trainable_param_in_layer1</span>
<span style="color: #888888"># layer1: (trainable_param_in_layer1*x)+2</span>

<span style="color: #888888"># @ Pass input_image into layer1 and get output_of_layer_1</span>
output_of_layer_1<span style="color: #333333">=</span>(tensor_after_layer1<span style="color: #333333">*</span>input_image)<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">2</span>

<span style="color: #888888"># @ Create loss function layer</span>
loss: loss_value<span style="color: #333333">=</span>output_of_layer_1<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">100</span>
</pre></div>
<xmp>
================================================================================
@ To update network 
(in this case, there is only one parameter trainable_param_in_layer1),
you need to calculate $$$\frac{\partial \text{loss_value}}{\partial \text{trainable_param_in_layer1}}$$$

@ In PyTorch, you can do it by using
</xmp>
<!-- loss_value.backward() -->
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">loss_value<span style="color: #333333">.</span>backward()
</pre></div>
<xmp>
================================================================================
@ And gradient of loss_value wrt trainable_param_in_layer1 is stored 
into trainable_param_in_layer1.grad

@ Note that there are 2 operations in your whole neural network
One operation is in layer1 and another operation is in loss function layer

You can't directly calculate $$$\dfrac{\partial \text{loss_value}}{\partial \text{trainable_param_in_layer1}}$$$
To calculate $$$\dfrac{\partial \text{loss_value}}{\partial \text{trainable_param_in_layer1}}$$$,
you should you chain rule, for example,
$$$\dfrac{\partial \text{loss_value}}{\partial \text{trainable_param_in_layer1}} = \dfrac{\partial \text{loss_value}}{\partial \text{output_of_layer1}} \times \dfrac{\partial \text{output_of_layer1}}{\partial \text{trainable_param_in_layer1}}$$$
================================================================================
@ The contribution of torch.autograd.Variables is 
that it calculates that gradient $$$\dfrac{\partial \text{loss_value}}{\partial \text{trainable_param_in_layer1}}$$$
on behalf of you who is originally supposed to manually perform multiple calculations 
in chain rule to find one gradient
================================================================================
Another similar example to above contents

</xmp>
<!-- x=torch.ones(3) -->
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">x<span style="color: #333333">=</span>torch<span style="color: #333333">.</span>ones(<span style="color: #0000DD; font-weight: bold">3</span>)
</pre></div>
<xmp>

</xmp>
<!-- y=Variable(x,requires_grad=True)
y=(x**2)
print(y.data)
print(y.grad)
print(y.grad_fn) -->
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">y<span style="color: #333333">=</span>Variable(x,requires_grad<span style="color: #333333">=</span><span style="color: #008800; font-weight: bold">True</span>)
y<span style="color: #333333">=</span>(x<span style="color: #333333">**</span><span style="color: #0000DD; font-weight: bold">2</span>)
<span style="color: #007020">print</span>(y<span style="color: #333333">.</span>data)
<span style="color: #007020">print</span>(y<span style="color: #333333">.</span>grad)
<span style="color: #007020">print</span>(y<span style="color: #333333">.</span>grad_fn)
</pre></div>
<xmp>

$$$z=3\times x^{2}$$$
</xmp>
<!-- z=y*3
print(z.data)
print(z.grad)
print(z.grad_fn) -->
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">z<span style="color: #333333">=</span>y<span style="color: #333333">*</span><span style="color: #0000DD; font-weight: bold">3</span>
<span style="color: #007020">print</span>(z<span style="color: #333333">.</span>data)
<span style="color: #007020">print</span>(z<span style="color: #333333">.</span>grad)
<span style="color: #007020">print</span>(z<span style="color: #333333">.</span>grad_fn)
</pre></div>
<xmp>


grad=torch.Tensor([0.1,1,10])

z.backward(grad)

z.backward()

Manual calculation
$$$z=3\times x^{2}$$$
$$$\frac{\partial{z}}{\partial{x}} = 3\times 2\times x$$$
since x=1, if you put x=1 into $$$3\times 2\times x$$$
you can get $$$\frac{\partial{z}}{\partial{x}}=6$$$

print(x.data)
print(x.grad)
print(x.grad_fn)

Above x.grad is generated from grad=torch.Tensor([0.1,1,10]) * 6

</xmp>
   </BODY>
</HTML>
