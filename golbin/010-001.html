<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
010-001. rnn, mnist
<xmp>
import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets("/media/young/5e7be152-8ed5-483d-a8e8-b3fecfa221dc/programming materials/mnist/data/",one_hot=True)

# @
# You configure options
learning_rate=0.001
total_epoch=30
batch_size=128

# Since rnn deals with time series data,
# you should define number of data,
# which you will provide at one time,
# and how many steps entire data has
# So,you will define number of width pixels as n_input,
# and number of height pixels as n_step
n_input=28
n_step=28
n_hidden=128
# 0~9
n_class=10

# @
# You build neural network model

# [n,28,28]
X=tf.placeholder(tf.float32,[None,n_step,n_input])
# [n,10]
Y=tf.placeholder(tf.float32,[None,n_class])
# [128,10]
W=tf.Variable(tf.random_normal([n_hidden,n_class]))
# [10]
b=tf.Variable(tf.random_normal([n_class]))

# You create cell which will be used in rnn
# Other kinds of cell you can use are BasicRNNCell,BasicLSTMCell,GRUCell
cell=tf.nn.rnn_cell.BasicRNNCell(n_hidden)

# You create rnn
# You actually should have done following steps
# states=tf.zeros(batch_size)
# for i in range(n_step):
#     outputs,states=cell(X[[:,i]],states)
# ...
# But,when you use tf.nn.dynamic_rnn(),
# it automatically creates rnn like tf.nn.conv2d() for cnn
# outputs is y
# states is $$$h_{t}$$$
outputs,states=tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)

# You should convert final result into following format of Y
# Format of Y is [batch_size,n_class]
# So,you should convert format of outputs into following format
# outputs : [batch_size,n_step,n_hidden]
#        -> [n_step,batch_size,n_hidden]
#        -> [batch_size,n_hidden]
# You change order of outputs
outputs=tf.transpose(outputs,[1,0,2])
outputs=outputs[-1]
model=tf.matmul(outputs,W)+b

# You create lost function in softmax_cross_entropy_with_logits()
cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model,labels=Y))
# You create optimizer
optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)

# @
# You train rnn model

sess=tf.Session()
sess.run(tf.global_variables_initializer())
# If num_examples is 10000,if batch_size is 100,
# total_batch will be 100
total_batch=int(mnist.train.num_examples/batch_size)

for epoch in range(total_epoch):
    total_cost=0
    for i in range(total_batch):
        batch_xs,batch_ys=mnist.train.next_batch(batch_size)
        # You reshape batch_xs into (batch_size,n_step,n_input),
        # to make batch_xs as rnn input data shape
        batch_xs=batch_xs.reshape((batch_size,n_step,n_input))
        # You obtain loss value
        _,cost_val=sess.run([optimizer,cost],feed_dict={X:batch_xs,Y:batch_ys})
        # You accumulate cost_val into total_cost
        total_cost+=cost_val

    print('Epoch:','%04d'%(epoch+1),'Average of loss=','{:.3f}'.format(total_cost/total_batch))

print('Completed optimization')

# @
# You see result
# You extract highest value from prediction values
# You extract highest value from label values
is_correct=tf.equal(tf.argmax(model,1),tf.argmax(Y,1))
accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))

test_batch_size=len(mnist.test.images)
test_xs=mnist.test.images.reshape(test_batch_size,n_step,n_input)
test_ys=mnist.test.labels

print('Accuracy:',sess.run(accuracy,feed_dict={X:test_xs,Y:test_ys}))

</xmp>
   </BODY>
</HTML>
