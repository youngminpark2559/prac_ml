<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
010-003. seq2seq, translator
<xmp>
# Seq2Seq is model which create and train sequence
# Seq2Seq is used for chatbot,translation,image captioning

# @
# You will create translator which translates english word into korean word
import tensorflow as tf
import numpy as np

# S is symbol representing start point of decoding's input
# E is symbol representing end point of decoding's output
# P is symbol representing filled character in empty sequence,
# if size of word is smaller than size of time step of current batch data
# For example,if maxiaml size of current batch data is 4,
#       word -> ['w','o','r','d']
#       to   -> ['t','o','P','P']
char_arr=[c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']
# < ['S',
# <  'E',
# <  'P',
# <  'a',
# <  'b',
# <  'c',
# <  'd',
# <  'e',
# <  'f',
# <  'g',
# <  'h',
# <  'i',
# <  'j',
# <  'k',
# <  'l',
# <  'm',
# <  'n',
# <  'o',
# <  'p',
# <  'q',
# <  'r',
# <  's',
# <  't',
# <  'u',
# <  'v',
# <  'w',
# <  'x',
# <  'y',
# <  'z',
# <  '단',
# <  '어',
# <  '나',
# <  '무',
# <  '놀',
# <  '이',
# <  '소',
# <  '녀',
# <  '키',
# <  '스',
# <  '사',
# <  '랑']
num_dic={n: i for i,n in enumerate(char_arr)}
# < {'E': 1,
# <  'P': 2,
# <  'S': 0,
# <  'a': 3,
# <  'b': 4,
# <  'c': 5,
# <  'd': 6,
# <  'e': 7,
# <  'f': 8,
# <  'g': 9,
# <  'h': 10,
# <  'i': 11,
# <  'j': 12,
# <  'k': 13,
# <  'l': 14,
# <  'm': 15,
# <  'n': 16,
# <  'o': 17,
# <  'p': 18,
# <  'q': 19,
# <  'r': 20,
# <  's': 21,
# <  't': 22,
# <  'u': 23,
# <  'v': 24,
# <  'w': 25,
# <  'x': 26,
# <  'y': 27,
# <  'z': 28,
# <  '나': 31,
# <  '녀': 36,
# <  '놀': 33,
# <  '단': 29,
# <  '랑': 40,
# <  '무': 32,
# <  '사': 39,
# <  '소': 35,
# <  '스': 38,
# <  '어': 30,
# <  '이': 34,
# <  '키': 37}
dic_len=len(num_dic)
# < 41

# This is train dataset for translator for english to korean
seq_data=[['word','단어'],['wood','나무'],
            ['game','놀이'],['girl','소녀'],
            ['kiss','키스'],['love','사랑']]

def make_batch(seq_data):
    input_batch=[]
    output_batch=[]
    target_batch=[]

    for seq in seq_data:
        # seq[0] is word
        # [25,17,20,6]
        input=[num_dic[n] for n in seq[0]]
        # Output will be input data for decoder
        # seq[1] is 단어
        # S단어
        # [0,29,30]
        output=[num_dic[n] for n in ('S' + seq[1])]
        # target=[num_dic[n] for n in ("단어E")]
        # < [29,30,1]
        target=[num_dic[n] for n in (seq[1] + 'E')]

        # Input batch should be one-hot-encoding
        input_batch.append(np.eye(dic_len)[input])
        # Output batch should be one-hot-encoding
        output_batch.append(np.eye(dic_len)[output])
        # Prediction and label don't need to be one-hot-encoding,
        # because you will use sparse_softmax_cross_entropy_with_logits(),
        # as loss function
        target_batch.append(target)

    return input_batch,output_batch,target_batch

# You configure options
learning_rate=0.01
n_hidden=128
total_epoch=100
# Since input and output are one-hot-encoding,
# size of them are same
n_class=n_input=dic_len

# @
# You build nn model

# In case of Seq2Seq model,
# format of encoder's input and format of decoder's input are same
# [None,None,n_input]=[batch size,time steps,input size]
enc_input=tf.placeholder(tf.float32,[None,None,n_input])
dec_input=tf.placeholder(tf.float32,[None,None,n_input])
# [None,None]=[batch size,time steps]
targets=tf.placeholder(tf.int64,[None,None])

# You build encoder cell
with tf.variable_scope('encode'):
    enc_cell=tf.nn.rnn_cell.BasicRNNCell(n_hidden)
    enc_cell=tf.nn.rnn_cell.DropoutWrapper(enc_cell,output_keep_prob=0.5)

    outputs,enc_states=tf.nn.dynamic_rnn(enc_cell,enc_input,dtype=tf.float32)

# You build decoder cell
with tf.variable_scope('decode'):
    dec_cell=tf.nn.rnn_cell.BasicRNNCell(n_hidden)
    dec_cell=tf.nn.rnn_cell.DropoutWrapper(dec_cell,output_keep_prob=0.5)

    # Most important point of Seq2Seq concept is
    # that you will input last state value of encoder initial_state=enc_states,
    # into decoder's initial state
    # You build recurrent nn
    outputs,dec_states=tf.nn.dynamic_rnn(dec_cell,dec_input,
                                        initial_state=enc_states,
                                        dtype=tf.float32)
# I create model
model=tf.layers.dense(outputs,n_class,activation=None)

# I create loss function
cost=tf.reduce_mean(
            tf.nn.sparse_softmax_cross_entropy_with_logits(
                logits=model,labels=targets))

# I create optimizer
optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)

# @
# I train neural network model
sess=tf.Session()
sess.run(tf.global_variables_initializer())

input_batch,output_batch,target_batch=make_batch(seq_data)

for epoch in range(total_epoch):
    _,loss=sess.run([optimizer,cost],
                    feed_dict={enc_input:input_batch,
                               dec_input:output_batch,
                               targets:target_batch})

    print('Epoch:','%04d'%(epoch+1),'cost =','{:.6f}'.format(loss))

print('Completed optimization')


# @
# You can test translator

# This method takes word and predict translated word
def translate(word):
    # This model uses [englishword,koreanword],
    # as input data and output data
    # But at prediction time, this model doesn't know korean word
    # So, you should fill input value and output value of decoder,
    # with meanless characters like P
    # ['word','PPPP']
    seq_data=[word,'P'*len(word)]

    input_batch,output_batch,target_batch=make_batch([seq_data])

    # Prediction will be format of [batch size,time step,input]
    # You extract highest value from input dimension(2) source
    # Then, you will get word which has highest probability
    prediction=tf.argmax(model,2)

    result=sess.run(prediction,
                    feed_dict={enc_input:input_batch,
                                dec_input:output_batch,
                                targets:target_batch})

    # You create character array,
    # by bringing character corresponding index number of result
    decoded=[char_arr[i] for i in result[0]]

    # You remove all words after "E"
    # Then, you create string
    end=decoded.index('E')
    translated=''.join(decoded[:end])

    return translated

print('\n=== Translation test ===')

print('word ->',translate('word'))
print('wodr ->',translate('wodr'))
print('love ->',translate('love'))
print('loev ->',translate('loev'))
print('abcd ->',translate('abcd'))


# < Epoch: 0001 cost = 3.719440
# < Epoch: 0002 cost = 2.826655
# < Epoch: 0003 cost = 1.577134
# < Epoch: 0004 cost = 0.949842
# < Epoch: 0005 cost = 0.630703
# < Epoch: 0006 cost = 0.409163
# < Epoch: 0007 cost = 0.270477
# < Epoch: 0008 cost = 0.136079
# < Epoch: 0009 cost = 0.179610
# < Epoch: 0010 cost = 0.122789
# < Epoch: 0011 cost = 0.315257
# < Epoch: 0012 cost = 0.209083
# < Epoch: 0013 cost = 0.073182
# < Epoch: 0014 cost = 0.158532
# < Epoch: 0015 cost = 0.073537
# < Epoch: 0016 cost = 0.131775
# < Epoch: 0017 cost = 0.024948
# < Epoch: 0018 cost = 0.016395
# < Epoch: 0019 cost = 0.023747
# < Epoch: 0020 cost = 0.006037
# < Epoch: 0021 cost = 0.007485
# < Epoch: 0022 cost = 0.011314
# < Epoch: 0023 cost = 0.006030
# < Epoch: 0024 cost = 0.004620
# < Epoch: 0025 cost = 0.009484
# < Epoch: 0026 cost = 0.002795
# < Epoch: 0027 cost = 0.002990
# < Epoch: 0028 cost = 0.004092
# < Epoch: 0029 cost = 0.009223
# < Epoch: 0030 cost = 0.003198
# < Epoch: 0031 cost = 0.003220
# < Epoch: 0032 cost = 0.002265
# < Epoch: 0033 cost = 0.003203
# < Epoch: 0034 cost = 0.002904
# < Epoch: 0035 cost = 0.003606
# < Epoch: 0036 cost = 0.001644
# < Epoch: 0037 cost = 0.001191
# < Epoch: 0038 cost = 0.003059
# < Epoch: 0039 cost = 0.002428
# < Epoch: 0040 cost = 0.003616
# < Epoch: 0041 cost = 0.001635
# < Epoch: 0042 cost = 0.000851
# < Epoch: 0043 cost = 0.001127
# < Epoch: 0044 cost = 0.001607
# < Epoch: 0045 cost = 0.002032
# < Epoch: 0046 cost = 0.003624
# < Epoch: 0047 cost = 0.001705
# < Epoch: 0048 cost = 0.000539
# < Epoch: 0049 cost = 0.001552
# < Epoch: 0050 cost = 0.000402
# < Epoch: 0051 cost = 0.000527
# < Epoch: 0052 cost = 0.000795
# < Epoch: 0053 cost = 0.004713
# < Epoch: 0054 cost = 0.000393
# < Epoch: 0055 cost = 0.000432
# < Epoch: 0056 cost = 0.001234
# < Epoch: 0057 cost = 0.001675
# < Epoch: 0058 cost = 0.000236
# < Epoch: 0059 cost = 0.000658
# < Epoch: 0060 cost = 0.001043
# < Epoch: 0061 cost = 0.000448
# < Epoch: 0062 cost = 0.000912
# < Epoch: 0063 cost = 0.000458
# < Epoch: 0064 cost = 0.000340
# < Epoch: 0065 cost = 0.000792
# < Epoch: 0066 cost = 0.000615
# < Epoch: 0067 cost = 0.001182
# < Epoch: 0068 cost = 0.000831
# < Epoch: 0069 cost = 0.000702
# < Epoch: 0070 cost = 0.001436
# < Epoch: 0071 cost = 0.000726
# < Epoch: 0072 cost = 0.001157
# < Epoch: 0073 cost = 0.001240
# < Epoch: 0074 cost = 0.000554
# < Epoch: 0075 cost = 0.001175
# < Epoch: 0076 cost = 0.000145
# < Epoch: 0077 cost = 0.000794
# < Epoch: 0078 cost = 0.002363
# < Epoch: 0079 cost = 0.000701
# < Epoch: 0080 cost = 0.000372
# < Epoch: 0081 cost = 0.001338
# < Epoch: 0082 cost = 0.002810
# < Epoch: 0083 cost = 0.000174
# < Epoch: 0084 cost = 0.000987
# < Epoch: 0085 cost = 0.000428
# < Epoch: 0086 cost = 0.001124
# < Epoch: 0087 cost = 0.000337
# < Epoch: 0088 cost = 0.000301
# < Epoch: 0089 cost = 0.000216
# < Epoch: 0090 cost = 0.000842
# < Epoch: 0091 cost = 0.000508
# < Epoch: 0092 cost = 0.000203
# < Epoch: 0093 cost = 0.000245
# < Epoch: 0094 cost = 0.000154
# < Epoch: 0095 cost = 0.000282
# < Epoch: 0096 cost = 0.000100
# < Epoch: 0097 cost = 0.000246
# < Epoch: 0098 cost = 0.000466
# < Epoch: 0099 cost = 0.000794
# < Epoch: 0100 cost = 0.000507
# < Completed optimization
# < 
# < === Translation test ===
# < word -> 단어
# < wodr -> 나무
# < love -> 사랑
# < loev -> 사랑
# < abcd -> 놀이

</xmp>
   </BODY>
</HTML>
