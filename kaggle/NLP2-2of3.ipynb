{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/corazzon/KaggleStruggle/blob/master/word2vec-nlp-tutorial/tutorial-part-2.ipynb  \n",
    "[NLP 2](2/3) Word2Vec을 Gensim을 통해 벡터화하고 t-SNE로 시각화 - IMDB 영화 리뷰 분석 캐글 머신러닝(기계학습)  \n",
    "\n",
    "@  \n",
    "I vectorize words by Word2Vec which is one of deep learning techniques.  \n",
    "I visualize vectorized word data by t-SNE.  \n",
    "I use hybrid methodology using both deep learning and random forest in supervised learning.  \n",
    "  \n",
    "@  \n",
    "Word2Vec(Word embedding To Vector).  \n",
    "Computer only can recognize numbers.  \n",
    "Character and image are saved as binary file.  \n",
    "I vectorized words by \"bag of word\" methodology to make computer understand words.  \n",
    "  \n",
    "@  \n",
    "Vectorizing by \"one hot encoding\" and \"bag of word\" makes too big and sparse vector.  \n",
    "It reduces perfomance in neural net.  \n",
    "  \n",
    "@  \n",
    "Word2Vec uses idea that the meaning of some word should be similar with the words around that word.  \n",
    "When I use around words as label for the specific word when I train it.  \n",
    "The process of Word2Vec is mapping word to \"dense vector\" containing meaning.  \n",
    "Word2Vec uses similarity between words, so, it can understand the relation between \"paris and france\" and \"berlin and germany\".  \n",
    "  \n",
    "@  \n",
    "![word2vec](https://1.bp.blogspot.com/-Q7F8ulD6fC0/UgvnVCSGmXI/AAAAAAAAAbg/MCWLTYBufhs/s1600/image00.gif)  \n",
    "이미지 출처 : https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html  \n",
    "  \n",
    "You can see the process of word embedding in visualization in real time : [word embedding visual inspector](https://ronxin.github.io/wevi/)  \n",
    "  \n",
    "@  \n",
    "![CBOW와 Skip-Gram](https://i.imgur.com/yXY1LxV.png)  \n",
    "출처 : https://arxiv.org/pdf/1301.3781.pdf  \n",
    "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.  \n",
    "  \n",
    "  \n",
    "@  \n",
    "There are 2 methods, CBOW, Skip-Gram.  \n",
    "CBOW(continuous bag-of-words) predicts one word by entire text, so, it's beneficial to use this for small entire data.  \n",
    "  \n",
    "@  \n",
    "The example of CBOW fitted task is predicting word for the blank in the simple sentence.  \n",
    "<pre>  \n",
    "1. __ has good taste   \n",
    "2. Riding __ is fun   \n",
    "3. Since I ate food 2 __, I'm feeling __ ache  \n",
    "</pre>  \n",
    "  \n",
    "@  \n",
    "Skip-Gram is predicting original word from target words.  \n",
    "As opposed to CBOW, Skip-Gram processes a pair of \"context-target\" as a new finding, and it's beneficial to use this when you have large data set.  \n",
    "  \n",
    "@  \n",
    "You can use Skip-gram to predict word which can be fit around follwing marked word.  \n",
    "<pre>  \n",
    "1. *Apple* has good taste   \n",
    "2. Riding *bike* is fun   \n",
    "3. Since I ate food 2 *times*, I'm feeling *stomach* ache  \n",
    "</pre>  \n",
    "  \n",
    "@  \n",
    "Word2Vec 참고자료    \n",
    "[word2vec 모델 · 텐서플로우 문서 한글 번역본](https://tensorflowkorea.gitbooks.io/tensorflow-kr/g3doc/tutorials/word2vec/)  \n",
    "[Word2Vec으로 문장 분류하기 · ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)  \n",
    "  \n",
    "[Efficient Estimation of Word Representations in  \n",
    "Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf)  \n",
    "[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)  \n",
    "[CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/syllabus.html)  \n",
    "[Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
    "  \n",
    "  \n",
    "@  \n",
    "Gensim  \n",
    "[gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)  \n",
    "[gensim: Tutorials](https://radimrehurek.com/gensim/tutorial.html)  \n",
    "[한국어와 NLTK, Gensim의 만남 - PyCon Korea 2015](https://www.lucypark.kr/docs/2015-pyconkr/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력이 너무 길어지지 않게하기 위해 찍지 않도록 했으나 \n",
    "# 실제 학습 할 때는 아래 두 줄을 주석처리 하는 것을 권장한다.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(25000, 2)\n",
      "(50000, 2)\n",
      "25000\n",
      "25000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train = pd.read_csv('D:/chromedown/labeledTrainData.tsv', \n",
    "                    header=0, delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('D:/chromedown/testData.tsv', \n",
    "                   header=0, delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv('D:/chromedown/unlabeledTrainData.tsv', \n",
    "                              header=0, delimiter='\\t', quoting=3)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(unlabeled_train.shape)\n",
    "\n",
    "print(train['review'].size)\n",
    "print(test['review'].size)\n",
    "print(unlabeled_train['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see there is no sentiment data in test data unlike train data\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KaggleWord2VecUtility import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'all',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'going',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'with']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KaggleWord2VecUtility.review_to_wordlist(train['review'][0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-872e30cda2f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     sentences += KaggleWord2VecUtility.review_to_sentences(\n\u001b[1;32m----> 6\u001b[1;33m         review, remove_stopwords=False)\n\u001b[0m",
      "\u001b[1;32mD:\\repo\\pracdatascience\\KaggleWord2VecUtility.py\u001b[0m in \u001b[0;36mreview_to_sentences\u001b[1;34m(review, remove_stopwords)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# 1. Use the NLTK tokenizer to split the paragraph into sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mraw_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# 2. Loop over each sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = []\n",
    "for review in train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(\n",
    "        review, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(\n",
    "        review, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@  \n",
    "Gensim\n",
    "[gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "@  \n",
    "Parameters of Word2Vec model  \n",
    "\n",
    "@  \n",
    "architecture : architecture option is given by skip-gram (default) or CBOW.  \n",
    "skip-gram (default) is slow but better result.  \n",
    "  \n",
    "learning algorith : hierarchical softmax (default) or negative sampling.  \n",
    "default one if working well.  \n",
    "  \n",
    "@  \n",
    "down sampling for frequently appearing word : Google document recommends .00001 ~ .001.  \n",
    "  \n",
    "@  \n",
    "dimensionality of vector of word  \n",
    "  \n",
    "@  \n",
    "context / window size  \n",
    "  \n",
    "@  \n",
    "minimal number of word  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I configure value of parameters.\n",
    "num_features = 300 # This is dimensionality of vector for words\n",
    "min_word_count = 40 # This is minimum number of characters\n",
    "num_workers = 4\n",
    "context = 10 \n",
    "downsampling = 1e-3\n",
    "\n",
    "# I initialize and make it learn\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료 되면 필요없는 메모리를 unload 시킨다.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = '300features_40minwords_10text'\n",
    "# model_name = '300features_50minwords_20text'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 결과 탐색 \n",
    "Exploring the Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도가 없는 단어 추출\n",
    "model.wv.doesnt_match('man woman child kitchen'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 유사한 단어를 추출\n",
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"film\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar(\"happy\")\n",
    "model.wv.most_similar(\"happi\") # stemming 처리 시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec으로 벡터화 한 단어를 t-SNE 를 통해 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim \n",
    "import gensim.models as g\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "model_name = '300features_40minwords_10text'\n",
    "model = g.Doc2Vec.load(model_name)\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "print(len(X))\n",
    "print(X[0][:10])\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# 100개의 단어에 대해서만 시각화\n",
    "X_tsne = tsne.fit_transform(X[:100,:])\n",
    "# X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_tsne, index=vocab[:100], columns=['x', 'y'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(40, 20)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    This is a function which find mean value of word vector from given sentence.\n",
    "    \"\"\"\n",
    "    # In advance, I initialize matrix by 0 to enhance the process speed.\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "\n",
    "    nwords = 0.\n",
    "    # Index2word is a list containing the name of word which is located in a dictionary of model.\n",
    "    # I convert index2word as set data type.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # I add words into features if they're involved in model dictionary as I go with \"for loop\".\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # I calculate the mean value of feature vector by deviding feature vector by the number of workds.\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # I calculate the mean feature vector of each list of review word.\n",
    "    # And then I return 2D numpy array.\n",
    "    \n",
    "    counter = 0.\n",
    "    \n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "       # 매 1000개 리뷰마다 상태를 출력\n",
    "       if counter%1000. == 0.:\n",
    "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       \n",
    "       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use 4 workers by using multiple threads.\n",
    "# I return clean_reviews\n",
    "def getCleanReviews(reviews):\n",
    "    clean_reviews = []\n",
    "    clean_reviews = KaggleWord2VecUtility.apply_by_multiprocessing(reviews[\"review\"], KaggleWord2VecUtility.review_to_wordlist, workers=4)\n",
    "    return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time trainDataVecs = getAvgFeatureVecs(getCleanReviews(train), model, num_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time testDataVecs = getAvgFeatureVecs(getCleanReviews(test), model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time forest = forest.fit(trainDataVecs, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "%time score = np.mean(cross_val_score(forest, trainDataVecs, train['sentiment'], cv=10, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict( testDataVecs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv('data/Word2Vec_AverageVectors_{0:.5f}.csv'.format(score), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 300features_40minwords_10text 일 때 0.90709436799999987\n",
    "* 300features_50minwords_20text 일 때 0.86815798399999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sentiment = output['sentiment'].value_counts()\n",
    "print(output_sentiment[0] - output_sentiment[1])\n",
    "output_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(train['sentiment'], ax=axes[0])\n",
    "sns.countplot(output['sentiment'], ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "544/578"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
